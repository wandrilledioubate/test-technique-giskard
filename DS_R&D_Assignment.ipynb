{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical exercise - Data scientist intern @ Giskard\n",
    "\n",
    "Hi! As part of our recruitment process, we’d like you to complete the following technical test in 10 days. Once you finish the exercise, you can send your notebook or share your code repository by email (matteo@giskard.ai). If you want to share a private GitHub repository, make sure you give read access to `mattbit`.\n",
    "\n",
    "If you have problems running the notebook, get in touch with Matteo at matteo@giskard.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn datasets transformers torch \"giskard>=2.0.0b\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Code review\n",
    "\n",
    "Your fellow intern is working on securing our API and wrote some code to generate secure tokens. You have been asked to review their code and make sure it is secure and robust. Can you spot the problem and write a short feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1IKnhie5N9EOGUoW5BZ\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import secrets\n",
    "\n",
    "ALPHABET = string.ascii_letters + string.digits\n",
    "\n",
    "def generate_secret_key_using_secrets(size: int = 20):\n",
    "    \"\"\"Generates a cryptographically secure random token using secrets module.\"\"\"\n",
    "    token = \"\".join(secrets.choice(ALPHABET) for _ in range(size))\n",
    "    return token\n",
    "\n",
    "# function testing\n",
    "secret_key = generate_secret_key_using_secrets()\n",
    "print(secret_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate security tokens, the library 'random' is clearly not the most secure. So I will make a small benchmark of libraries that would be more suitable to generate secure tokens. \n",
    "\n",
    "1) Secrets\n",
    "    - Advantage: secrets is simple to use and is part of the standard Python library, which means there is no need to install third-party libraries.\n",
    "    - Disadvantage: It does not offer as many advanced features as some other libraries, which can be limiting in complex use cases.\n",
    "\n",
    "\n",
    "2) PyJWT \n",
    "    - Advantage: PyJWT facilitates the creation and verification of JWT, a widely used standard for authentication and authorization, facilitating interoperability with other JWT-supported services and libraries.\n",
    "    - Disadvantage: Key management and configuration of JWT signature algorithms can be complex, especially for beginners.\n",
    "\n",
    "\n",
    "3) UUID\n",
    "    - Advantage: uuid generated UUIs are universally unique, making them suitable for many applications.\n",
    "    - Disadvantage: UUIDs have a fixed length, which may not be suitable for all situations, as variable length tokens may be required.\n",
    "\n",
    "\n",
    "4) Cryptography \n",
    "    - Advantage: cryptography offers advanced security and key management capabilities, making it an excellent choice for applications requiring a high level of security.\n",
    "    - Disadvantage: Because of its power, cryptography can be more complex to use for simple tasks. It can be oversized for simple token generation needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of library will depend on the needs. If simplicity is your priority, secrets or uuid may be enough. If you need JWT, PyJWT is a good choice. If you have high security requirements, cryptography may be the best option. Be sure to weigh these advantages and disadvantages according to your context of use.\n",
    "\n",
    "So I decided to use the 'secrets' library and declare the 'ALPHABET' variable with the 'string' library and ASCII code to be more readable and not exclude capital letters :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: High dimensions\n",
    "\n",
    "Matteo, our ML researcher, is struggling with a dataset of 40-dimensional points. He’s sure there are some clusters in there, but he does not know how many. Can you help him find the correct number of clusters in this dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters in a data set with 40-dimensional points can be more complex than with two-dimensional data. The most common method for choosing the right number of clusters for large data is silhouette analysis and K-means algorithm. For this, I will use the Scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like there are 11 clusters.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "x = np.load(\"points_1.npy\")\n",
    "\n",
    "minimum_clusters = 2\n",
    "maximum_clusters = 30\n",
    "\n",
    "silhouette_scores = []\n",
    "optimal_clusters = None\n",
    "optimal_score = -1.0\n",
    "\n",
    "for n_clusters in range(minimum_clusters, maximum_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10,random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(x)\n",
    "    silhouette_avg = silhouette_score(x, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    if silhouette_avg > optimal_score:\n",
    "            optimal_score = silhouette_avg\n",
    "            optimal_clusters = n_clusters\n",
    "    \n",
    "print(f\"It looks like there are {optimal_clusters} clusters.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matteo is grateful for how you helped him with the cluster finding, and he has another problem for you. He has another high-dimensional dataset, but he thinks that those points could be represented in a lower dimensional space. Can you help him determine how many dimensions would be enough to well represent the data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how many dimensions would be sufficient to properly represent the data, I will use a principal component analysis (PCA) technique to reduce the size of the dataset while preserving a sufficiently high proportion of the total variance, always using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like the data is 25-dimensional\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x = np.load(\"points_2.npy\")\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(x)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "threshold = 0.95  \n",
    "num_dimensions = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "print(f\"It looks like the data is {num_dimensions}-dimensional\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to take a 95% cumulative variance threshold in order to minimize the loss of information. In doing so, we have a 25-dimensional dataset. For comparison, if I put cumulative variance threshold at 90%, I have a 13-dimensional dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Mad GPT\n",
    "\n",
    "Matteo is a good guy but he is a bit messy: he fine-tuned a GPT-2 model, but it seems that something went wrong during the process and the model became obsessed with early Romantic literature.\n",
    "\n",
    "Could you check how the model would continue a sentence starting with “Ty”? Could you recover the logit of the next best token? And its probability?\n",
    "\n",
    "You can get the model from the HuggingFace Hub as `mattbit/gpt2wb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Token: ger\n",
      "Logit: tensor(-16.2950)\n",
      "Probability: 99.19%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mattbit/gpt2wb\")\n",
    "\n",
    "# Definition and tokenization of the selected input text ('Ty')\n",
    "input_text = \"Ty\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generation of the next token while disabling the calculation of the gradient because we don't want to train the model. \n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "# Retrieving logits from the next token using the model output\n",
    "logits = output.logits[0, -1, :]\n",
    "\n",
    "# We get the index of the token with the highest logits \n",
    "next_token_index = logits.argmax().item()\n",
    "\n",
    "# Then we calculate the logits value of this token\n",
    "next_token_logit = logits[next_token_index]\n",
    "\n",
    "# We calculate the probability that this token is generated by the model with the function 'F.softmax' because the logit of this token is negative. \n",
    "next_token_probability = F.softmax(logits, dim=0)[next_token_index].item() * 100\n",
    "\n",
    "# We end up decoding the token to find what will come after 'Ty' according to the model\n",
    "next_token = tokenizer.decode(next_token_index)\n",
    "\n",
    "\n",
    "print(\"Next Token:\", next_token)\n",
    "print(\"Logit:\", next_token_logit)\n",
    "print(f\"Probability: {next_token_probability:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Not bad reviews\n",
    "\n",
    "\n",
    "We trained a random forest model to predict if a film review is positive or negative. Here is the training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_data = datasets.load_dataset(\"sst2\", split=\"train[:20000]\").to_pandas()\n",
    "valid_data = datasets.load_dataset(\"sst2\", split=\"validation\").to_pandas()\n",
    "\n",
    "# Prepare model\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "preprocessor = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase=False)\n",
    "classifier = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n",
    "\n",
    "model = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "# Train\n",
    "X = train_data.sentence\n",
    "y = train_data.label\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\n",
    "    \"Training complete.\",\n",
    "    \"Accuracy:\",\n",
    "    model.score(valid_data.sentence, valid_data.label),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it works quite well, but we noticed it has some problems with reviews containing negations, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels are:\n",
    "# 1 = Positive, 0 = Negative\n",
    "\n",
    "# this returns positive, that’s right!\n",
    "assert model.predict([\"This movie is good\"]) == [1]\n",
    "\n",
    "# negative! bingo!\n",
    "assert model.predict([\"This movie is bad\"]) == [0]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! uhm…\n",
    "assert model.predict([\"This movie is not bad at all!\"]) == [1]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! why?\n",
    "assert model.predict([\"This movie is not perfect, but very good!\"]) == [1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you help us understand what is going on? Do you have any idea on how to fix it?\n",
    "You can edit the code above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Model weaknesses\n",
    "\n",
    "\n",
    "The Giskard python library provides an automatic scanner to find weaknesses and vulnerabilities in ML models.\n",
    "\n",
    "Using this tool, could you identify some issues in the movie classification model above? Can you propose hypotheses about what is causing these issues?\n",
    "\n",
    "Then, choose one of the issues you just found and try to improve the model to mitigate or resolve it — just one, no need to spend the whole weekend over it!\n",
    "\n",
    "You can find a quickstart here: https://docs.giskard.ai/en/latest/getting-started/quickstart.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
